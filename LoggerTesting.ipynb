{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal abstraction: \n",
    "--The user specifies some \"frequency\" (in seconds, examples, batches, or epochs) at which certain metrics are calculated (can use different units, freqs). \n",
    "--Metrics can be built-in or custom (user provides a func for calculating them, given a model and DataLoader) \n",
    "--Everything else (printing, logging to TB, checkpointing, etc.) operates over this metrics dict\n",
    "--For checkpointing, the user can specify which metric to use and whether to min or max it.\n",
    "\n",
    "full name of metrics is split/metric (e.g., valid/accuracy)\n",
    "split is assumed based on which logger is being used, assumed to be valid for checkpointer\n",
    "    but you can explicitly indicate otherwise if you'd like!\n",
    "\n",
    "all metrics (standard and custom) are dumped to metrics_dict\n",
    "all metrics values are pulled from metrics_dict\n",
    "report all custom metrics, only report standard metrics that are explicitly requested\n",
    "\n",
    "Train loss gets reported continuously with tqdm progress bar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic.generate import singletask_synthetic\n",
    "\n",
    "n = 10000\n",
    "m = 10\n",
    "k = 2\n",
    "D, L, X, Y, _ = singletask_synthetic(n, m, k)\n",
    "    \n",
    "from metal.utils import split_data\n",
    "Xs, Ys, Ls, Ds = split_data(X, Y, L, D, splits=[0.8, 0.1, 0.1], stratify_by=Y, seed=123)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "\n",
    "Y_train_ps = mv.predict_proba(Ls[0])\n",
    "# scores = mv.score((Ls[1], Ys[1]), metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lr(model, dataloader):\n",
    "    return {'lr': model.optimizer.param_groups[0]['lr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "No checkpoints will be saved in the first checkpoint_runway=3 iterations.\n",
      "[1 epo]: TRAIN: [loss=0.165, lr=0.010] VALID: [accuracy=0.989, precision=0.982]\n",
      "[2 epo]: TRAIN: [loss=0.150, lr=0.010] VALID: [accuracy=0.992, precision=1.000]\n",
      "[3 epo]: TRAIN: [loss=0.146, lr=0.010] VALID: [accuracy=0.993, precision=0.996]\n",
      "Saving model at iteration 3 with best score 0.146\n",
      "Saving model at iteration 3 with best score 0.083\n",
      "Saving model at iteration 3 with best score 0.082\n",
      "[4 epo]: TRAIN: [loss=0.147, lr=0.010] VALID: [accuracy=0.990, precision=0.988]\n",
      "[5 epo]: TRAIN: [loss=0.145, lr=0.010] VALID: [accuracy=0.996, precision=1.000]\n",
      "Restoring best model from iteration 3 with score 0.082\n",
      "Finished Training\n",
      "Accuracy: 0.992\n",
      "        y=1    y=2   \n",
      " l=1    488     3    \n",
      " l=2     5     504   \n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "\n",
    "end_model = EndModel([1000,10,2])\n",
    "end_model.train_model(\n",
    "    (Xs[0], Y_train_ps), \n",
    "    valid_data=(Xs[1], Ys[1]), \n",
    "    l2=0.01, \n",
    "    batch_size=16, \n",
    "    n_epochs=5, \n",
    "    progress_bar=False,\n",
    "    writer=None,\n",
    "    checkpoint=True,\n",
    "#     checkpoint_metric=\"train/loss\",\n",
    "#     checkpoint_metric_mode=\"min\",\n",
    "#     checkpoint_runway=3,\n",
    "    log_unit='epochs',\n",
    "    log_train_metrics=['train/loss'],\n",
    "    log_train_metrics_func=extract_lr,\n",
    "    log_valid_metrics=['accuracy','precision'],\n",
    "    log_train_every=1,\n",
    "    log_valid_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['valid', 'foo/bar']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_metric = \"valid/foo/bar\"\n",
    "split_metric.split(\"/\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
